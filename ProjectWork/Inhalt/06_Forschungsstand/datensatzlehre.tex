\section{Anforderungen an einen Datensatzes zur Erstellung eines Deep Learning Modells} \label{data}

\subsection*{Datensatzzusammensetzung}

Zum Erstellen und Auswählen eines \textit{Deep Learning} Modells wird der Datenbestand in der Regel in drei Kategorien unterteilt. Ein Datensatz wird für das Training des Modells verwendet. Durch das anschließende Anwenden des Modells auf zuvor ungesehene Daten, den Testdaten, wird der \textit{Verallgemeinerungsfehler} gemessen, der möglichst niedrig ausfallen sollte. Fällt der allgemeine Fehler während des Trainings niedrig aus, der \textit{Verallgemeinerungsfehler} während des Testdurchlaufs allerdings hoch, so liegt klassisches \textit{Overfitting} vor, die Trainingsdaten wurden auswendig gelernt \cite{AurelienGeron.2018}. 

Anschließend werden in mehreren Durchläufen die Hyperparameter des Trainingsprozesses angepasst, sodass letztendlich der \textit{Verallgemeinerungsfehler} für die Testdaten niedrig ausfällt. Kommt es anschließend zum Einsatz des Modells in der Produktivumgebung, so können trotz allem unerwartete Ergebnisse bezüglich des Abstraktionsvermögens des Modells auftreten, was daran liegt, dass das Modell allein auf die Testdaten hin optimiert wurde. Um dies zu vermeiden, wird ein dritter Datensatz, der Validierungsdatensatz, eingeführt. Mehrere Modelle werden dabei durch den Validierungsdatensatz getestet und das am besten abschneidende Modell mit dessen Hyperparametern ausgewählt. Der eigentliche Testdatendatz wird anschließend nur noch zur Abschätzung des \textit{Verallgemeinerungsfehlers} verwendet \cite{AurelienGeron.2018}. 

Oft wird der Trainingsdatensatz mit dem Validierungsdatensatz zum sogenannten \textit{Trainval} Datensatz zusammengeführt. Dies steht im Kontext des sogenannten \textit{K-Kreuzvalidier-""ungsverfahrens}. Dabei wird der \textit{Trainval} Datensatz in K gleich große, komplementäre Untermengen unterteilt. Eine dieser Untermengen dient anschließend als Validierungsdatensatz. Für jedes zu trainierende Modell mit unterschiedlichen Hyperparametern wird eine andere Untermenge als Validierungsdatensatz ausgewählt. Hierdurch steigt die Aussagekraft des Abstraktionsvermögens nach der Validierung und zudem müssen keine Trainingsdaten dauerhaft für die Validierung zurück gelegt werden. In der Regel werden 80\% der Gesamtdaten als \textit{Trainval} Datensatz verwendet \cite{AurelienGeron.2018}.

\subsection*{Qualität und Quantität der Daten}

Um ein funktionsfähiges Modell zu trainieren, muss der Datensatz einem gewissen Standard nachkommen. Demnach müssen die zu klassifizierenden Objekte vollständig im Bild enthalten und gut erkennbar sein. Zwar gibt es gerade im \textit{PascalVOC} Datensatzformat ebenso die Möglichkeit, Objekte als \glqq schwierig erkennbar\grqq{} zu markieren, dennoch sollen solche Objekte nicht die Mehrheit im gesamten Datensatz ausmachen. Auch die Aufnahme von Objekten in unterschiedlichen Umgebungen, Verdeckungsgraden, Entfernungen und Blicklagen fördert langfristig das Abstraktionsvermögen des Modells. 

Ebenso muss ein ausreichend großer Datensatz vorliegen, um das gewünschte Abstraktionsvermögen des Modells zu erreichen. Die Ergebnisse aus Abbildung \ref{result} wurden beispielsweise durch Kombination der \textit{Trainval} Datensätze von PascalVOC 2007 und 2012 erzielt und umfasst 16.551 Bilder im Trainingsverfahren \cite{ssd.20161229, MarkEveringham.20070607, MarkEveringham.20120521}. 

Unter Hinzunahme des COCO \textit{trainval135k} Datensatzes erreicht der \textit{SSD} sogar das beste Ergebnis aus der ursprünglichen Veröffentlichung mit einer durchschnittlichen Präzision von 81.6\% \cite{ssd.20161229}. 

\subsection*{Techniken zum Trainieren bei geringen Datenmengen}

Bei Betrachtung der obigen Ergebnisse wird schnell deutlich, dass für ein komplexeres \textit{Deep Learning} Modell ein umfangreicher Datensatz von Nöten ist. Allerdings gibt es zwei bekannte Techniken, wie auch mit kleineren Datenbeständen ein sehenswertes Ergebnis erzielt werden kann. 

Beim sogenannten \textit{Transfer Learning} können von einem bereits für ein ähnliches Problem trainiertes Modell die ersten Schichten des neuronalen Netzes für das neue Modell wiederverwendet werden. Die übernommenen Gewichtungen werden nicht mit trainiert. Neben einer kleineren Datenmenge zum Trainieren hat das \textit{Transfer Learning} ebenso den Vorteil das Training selbst zu beschleunigen \cite{AurelienGeron.2018}.

Eine weitere Technik beschreibt das künstliche Vergrößern des Datensatzes durch affine Transformationen wie Translation, Rotation oder Skalierung und wird \textit{Data Augmentation} genannt \cite{AurelienGeron.2018}.
