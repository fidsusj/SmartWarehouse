\section{Grundlagen zu Objektdetektoren} \label{basics}

Ein Anwendungsgebiet des \textit{Deep Learnings} beschreiben die Objektdetektoren, die im \textit{Smart Warehouse} Szenario zur Lokalisierung und Klassifizierung von Bestandsobjekten genutzt werden. Im folgenden Kapitel soll demnach der Grundbaustein von Objektdetektoren, das \textit{Convolutional Neural Network}, zunächst genauer betrachtet werden, bevor auf die gängigsten Objektdetektoren, die der \textit{Regional Convolutional Neural Networks} (R-CNN), der \textit{Single Shot MultiBox Detector} und der \textit{You Only Look Once} Ansatz im darauf folgenden Kapitel eingegangen wird. Auch wird die gängiste Metrik zum Vergleich von Objektdetektoren, die \textit{mean Average Precision} eingeführt.

\subsection*{Convolutional Neural Networks}

CNNs sind aus drei wesentlichen Bestandteilen aufgebaut. \textit{Convolutional Layer} erzeugen mehrere, übereinander gelegte \textit{Feature Maps}, die durch Anwendung mehrerer Filter, auch \textit{Kernel} genannt, auf ein Bild durch die mathematischen Faltungsoperation entstehen. Daher stammt der \glqq faltungsbedingten\grqq{} (engl.: convolutional) Charakter der Schicht. Die Filter werden auf kleinere Bereiche eines Bildes angewandt und sollen dabei Muster im Bild erkennen. Die Filter werden durch Gewichtungen implementiert, die im CNN trainiert werden müssen, was das \glqq Lernen\grqq{} im CNN darstellt. Ziel ist es also die optimalen Filter zu finden, die ein Objekt auf Bildern erkennen sollen. Selbstverständlich können auch weitere Filter trainiert werden, um mehrere Objekte zu erkennen. Dadurch, dass die Filter nur auf Teilbereiche des Bildes angewendet werden, ist somit nicht jede LTU mit allen LTUs der vorherigen Feature Map verbunden, man spricht hier auch von einem \glqq lokalen Wahrnehmungsfeld\grqq{}. Im Verlauf des CNNs werden kleinere Muster zu größeren Mustern abstrahiert und dabei das CNN durch sogenannte \textit{Pooling Layer} verkleinert. Ein einfaches Feed-Forward ANN mit \textit{Fully-Connected Layern} am Ende des CNNs trifft anschließend die Klassifikationsaussage des Bildes, also welches Objekt auf dem Bild gezeigt wird \cite{AurelienGeron.2018}. Auch zu CNNs können weitere Informationen im Anhang nachgelesen werden.