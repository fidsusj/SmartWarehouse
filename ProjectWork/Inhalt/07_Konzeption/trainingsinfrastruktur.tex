\section{Auswahl der Trainingsinfrastruktur} \label{infrastructure}

Bei der Auswahl der Trainingsinfrastruktur wurden zunächst die Cloud PaaS-Angebote in Betrachtung gezogen. Diese ermöglichen meist eine weit bessere Performance als lokales Training. Wichtig bei der Auswahl war hierbei 

\begin{itemize}
	\item möglichst niedrige Betriebskosten,
	\item ein diverses Angebot an Hardware-Beschleunigern und
	\item ein einfaches Aufsetzen der Trainingsinfrastruktur.
\end{itemize}

Insbesondere sollten die Testversionen der jeweiligen Angebote zu Nutze gemacht werden, um niedrige Betriebskosten zu erreichen. Manche Testversionen bieten hierbei ein Startkontingent, das je nach ausgewählter Hardware unterschiedlich schnell bei Nutzung verbraucht wird, bei anderen Cloud Anbietern wird die Hardwarekonfiguration vorgegeben, die anschließend nur für eine bestimmte Zeitdauer unter Last genutzt werden kann. In Tabelle \ref{table:comparison} sind die Ergebnisse der Untersuchung dargestellt. 

\begin{center}
	\begin{tabular}[H]{l|c|c|c|c|c}
		& AWS & GCP & Azure & FloydHub & Colab \\
		\hline
		Nutzungsrahmen & 50 Std. & 300\$ & 200\$ & 2 Std. & Keine Beschränkung \\
		Hardware-Beschleuniger & Nein & Ja & Ja & Ja & Ja \\
		Setup-Komplexität & Hoch & Mittel & Mittel & Einfach & Einfach \\
	\end{tabular}
	\captionof{table}{Kostenlose SaaS-Angebote der Cloud Anbieter}
	\label{table:comparison}
\end{center}

\textit{Amazon SageMaker} bietet hierbei für 50 Stunden eine \textit{ml.m4.xlarge} Instanz für Modelltrainingszwecke an \cite{AmazonWebServices.2020}. Da diese allerdings nur 4 vCPUs und 16 GiB Arbeitsspeicher umfasst, also keinerlei Cloud GPU als Hardwarebeschleuniger angeboten wird, wurde das Angebot wieder verworfen \cite{AmazonWebServices.20200314b}.

Auf Empfehlung wurde anschließend die \textit{GCP} betrachtet. Diese bietet mit 300\$ Startguthaben für 12 Monate ein lukratives Angebot zum Ausprobieren von beliebigen \textit{GCP} Produkten \cite{GoogleCloudPlatform.20200314b}. Die Benutzung der \textit{Deep Learning VM} bietet zudem eine native Unterstützung des \textit{PyTorch} Frameworks, was von der \textit{SSD} Implementierung genutzt wird, und zugleich eine Auswahl aus vier gängigen Cloud GPUs, der \textit{NVIDIA Tesla K80}, \textit{NVIDIA Tesla P100}, \textit{NVIDIA Tesla T4} und der \textit{NVIDIA Tesla V100}. Um die Konfiguration der \textit{Deep Learning VM} allerdings mit Auswahl einer Cloud GPU abschließen zu können, muss zunächst das mit dem Account verknüpfte Kontingent erhöht werden. Hierzu musste an das \textit{GCP} Support Team ein offizieller Antrag gestellt werden. Aufgrund der geringen Kaufhistorie wurde der Antrag allerdings abgelehnt. 

\textit{Microsoft Azure} bietet für 200\$ bei einer Laufzeit von 30 Tagen Zugang zu allen \textit{Microsoft Azure} Diensten \cite{MicrosoftAzure.2020}. Darunter gehört eine \textit{NC6} Instanz mit sechs vCPUs und einer \textit{NVIDIA Tesla K80} \cite{MicrosoftAzure.202003124}. Da \textit{Microsoft Azures} Angebot allerdings nur sehr oberflächlich beschrieben wurde, wurde sich letzten Endes auch gegen \textit{Microsoft Azure} entschieden. 

Als letzter Anbieter wurde \textit{FloydHub} getestet. Hervorzuheben ist die besonders einfache Vorgehensweise bei der Account Erstellung und dem Aufsetzen der Trainingsinfrastruktur, was bereits in Kapitel \ref{cloud} beschrieben wurde. \textit{FloydHub} bietet 20 Stunden CPU Trainingszeit bzw. 2 Stunden GPU Trainingszeit auf einer \textit{NVIDIA Tesla K80} \cite{FloydHub.2020}. Neben einer \textit{NVIDIA Tesla K80} konnte ebenso Trainingszeit auf einer \textit{NVIDIA Tesla V100} erworben werden. Zudem wurde das verwendete \textit{PyTorch} Framework unterstützt. Aufgrund der einfachen Handhabung wurde sich trotz der erhöhten Kosten für \textit{FloydHub} entschieden. 

Während des Trainings mit der \textit{NVIDIA Tesla K80} fiel allerdings auf, dass die Wahl dieser GPU keine großen Performance Verbesserungen gegenüber lokalem Training brachte. Während lokal innerhalb einer Stunde 16 Epochen durchlaufen werden konnten, waren dies bei \textit{Floydhub} hingegen nur 8 Epochen. Dies veranlasste eine Gegenüberstellung gängiger Cloud GPUs mit lokalen GPUs, allen voran den bereits vorhandenen Desktop-Grafikkarten \textit{NVIDIA GeForce GTX 1080} und \textit{NVIDIA GeForce RTX 2060 SUPER} (siehe Tabelle \ref{table:hardware}) \cite{TechPowerUp.20200209}.

\begin{center}
	\begin{tabular}[H]{l|c|c|c|c|c|c}
		& K80 & P100 & T4 & V100 & GTX 1080 & RTX 2060 SUPER \\
		\hline
		CUDA Cores & 2496 & 3584 & 2560 & 5120 & 2560 & 2176 \\
		Tensor Cores & / & / & 320 & 640 & / & 272 \\
		TeraFLOPS (Single Precision) & 4,113 & 9,526 & 8,141 & 14,13 & 9,784 & 7,377 \\
		Memory Bandwidth (GB/sec) & 240,6 & 732,2 & 320 & 897 & 345,6 & 448 \\
		Suggested Power Supply Unit & 700 & 600 & 350 & 600 & 450 & 450
	\end{tabular}
	\captionof{table}{Vergleich von GPUs nach Rechenleistung}
	\label{table:hardware}
\end{center}

Hierbei fällt auf, dass im Grad der Parallelisierung eine \textit{NVIDIA Tesla K80} zwar mit den vorhandenen lokalen Grafikkarten mithalten kann, in der Anzahl an Rechenoperationen pro Sekunden allerdings weit schlechter abschneidet. Im Vergleich zu einer \textit{NVIDIA GeForce GTX 1080} schneidet die \textit{NVIDIA Tesla K80} bezüglich der Rechenleistung weniger als halb so schnell ab, was auch erklärt, warum nur halb so viele Epochen in einer Stunde auf der \textit{FloydHub} Cloud trainiert werden konnten. Damit sich das Training in der Cloud nach Performance lohnt, muss demnach mindestens eine \textit{NVIDIA Tesla V100} verwendet werden. Da diese allerdings mit 42\$ für zehn Stunden mehr als dreimal so teuer als eine \textit{NVIDIA Tesla K80} für 12\$ ist und zusätzlich zu den GPU Kosten noch monatliche Account-Gebühren berechnet werden\footnote{Je nach Account kann eine unterschiedliche Anzahl an Projekten erstellt und Speicherplatz verwendet werden. Die \textit{Beginner} Ausstattung von einem Projekt und 10 GB Speicher ist allerdings kostenfrei.}, wurde sich nach nun nach Kosten-Nutzen Abwägung letzten Endes auf lokales Training festgelegt. Dies ist ebenso hinsichtlich des Trainings des \textit{YOLO} Objektdetektors besser, da das sehr spezifische \textit{Darknet} Framework, das in der Implementierung genutzt wird, bisher von noch keinem Cloud Anbieter unterstützt wurde. Das Trainieren des \textit{YOLO} Objektdetektors in der Cloud hätte demnach eine Umentscheidung auf eine Alternativ-Implementierung in beispielsweise \textit{TensorFlow} oder \textit{PyTorch} nötig gemacht. Werden noch andere Anpassungen in der Programmlogik mit einbezogen, z.B. dass ein Zugriff auf das Dateisystem beim Erstellen von Dateien in der \textit{SSD} Implementierung in der Cloud Umgebung nicht möglich ist, so kommen zusätzlich zeitliche Bedenken mit auf. Ein lokales Trainieren bietet unter den genannten Voraussetzungen somit eine weitaus bessere Umgebung.

Auch wurden Überlegungen zum Training in \textit{Google Colab} unternommen, da hier ein einfaches Training mit TPUs ermöglicht werden kann. Da allerdings in \textit{Google Colab} nach 90 Minuten ein Trainingsjob beendet wird und die Rechenresourcen neuen Nutzern zugewiesen werden, war diese Art des Trainings ebenfalls nicht möglich. 

Aus arbeitstechnischen Gründen ist anzumerken, dass die beiden Objektdetektoren \textit{SSD} und \textit{YOLO} lokal jeweils auf unterschiedlicher Hardware trainiert werden, \textit{SSD} auf einer \textit{NVIDIA GeForce GTX 1080} und \textit{YOLO} auf einer \textit{NVIDIA GeForce RTX 2060 SUPER} GPU.
